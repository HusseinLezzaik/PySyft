{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e7bbe-dd24-4dcc-a4ab-eaf3a9abf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "import unittest\n",
    "from uuid import uuid1\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch \n",
    "from torch.testing._internal.common_utils import TestCase, run_tests\n",
    "\n",
    "numpy_to_torch_dtype_dict = {\n",
    "    np.bool_      : torch.bool,\n",
    "    np.uint8      : torch.uint8,\n",
    "    np.int8       : torch.int8,\n",
    "    np.int16      : torch.int16,\n",
    "    np.int32      : torch.int32,\n",
    "    np.int64      : torch.int64,\n",
    "    np.float16    : torch.float16,\n",
    "    np.float32    : torch.float32,\n",
    "    np.float64    : torch.float32,  # float64 -> float32\n",
    "    np.complex64  : torch.complex64,\n",
    "    np.complex128 : torch.complex128\n",
    "}\n",
    "\n",
    "from torch.testing._internal.common_device_type import ops, instantiate_device_type_tests\n",
    "from torch.testing._internal.common_methods_invocations import op_db, DecorateInfo\n",
    "from torch.utils._pytree import tree_map\n",
    "\n",
    "import syft as sy\n",
    "from syft.core.tensor.autodp.phi_tensor import TensorWrappedPhiTensorPointer, TensorWrappedGammaTensorPointer\n",
    "sy.logger.remove()\n",
    "\n",
    "aten = torch.ops.aten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184e6a6-7745-4f78-85e6-d77da46c207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decompositions = dict()\n",
    "\n",
    "@register_func(aten.relu.default, decompositions)\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "@register_func(aten.addmm.default, decompositions)\n",
    "def addmm(bias, a, b):\n",
    "    return torch.mm(a, b) + bias\n",
    "\n",
    "@register_func(aten.sigmoid.default, decompositions)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(x * -1))\n",
    "\n",
    "@register_func(aten.sigmoid_backward.default, decompositions)\n",
    "def sigmoid_backward(grad_out, result):\n",
    "    # FIXME: DPTensor needs to be on the LHS always?\n",
    "    return grad_out * result * (result * -1 + 1)\n",
    "\n",
    "@register_func(aten.threshold_backward.default, decompositions)\n",
    "def threshold_backward(grad_out, self, threshold):\n",
    "    return grad_out * (self > threshold)\n",
    "\n",
    "def contig_strides_from_shape(shape):\n",
    "    return torch.zeros(shape).stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b6059-f3f1-438c-97ad-568ff892a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPTensor(torch.Tensor):\n",
    "    # This class wraps a TensorPointer so autograd, and other can be used\n",
    "    #\n",
    "    # TODO:\n",
    "    # - Think about the gamma case\n",
    "    # - nit: maybe use slots here to be more efficient\n",
    "    @staticmethod\n",
    "    def __new__(cls, pointer, requires_grad=False):\n",
    "        assert isinstance(pointer, TensorWrappedPhiTensorPointer) or \\\n",
    "            isinstance(pointer, TensorWrappedGammaTensorPointer)\n",
    "        r = torch.Tensor._make_wrapper_subclass(  # type: ignore[attr-defined]\n",
    "            cls,\n",
    "            pointer.public_shape,\n",
    "            strides=contig_strides_from_shape(pointer.public_shape),\n",
    "            storage_offset=0,  # elem.storage_offset(),\n",
    "            # TODO: clone storage aliasing\n",
    "            # FIXME: why don't we just return a np type instead of a string?\n",
    "            dtype=numpy_to_torch_dtype_dict[getattr(np, pointer.public_dtype)],\n",
    "            layout=torch.strided,\n",
    "            device=\"cpu\",  # elem.device\n",
    "            requires_grad=requires_grad\n",
    "        )\n",
    "        r.pointer = pointer\n",
    "\n",
    "        return r\n",
    "        \n",
    "    __torch_function__ = torch._C._disabled_torch_function_impl\n",
    "    \n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "        print(f\"__torch_dispatch__: {func.__module__}.{func.__name__}\")\n",
    "        def wrap(t):\n",
    "            if isinstance(t, TensorWrappedPhiTensorPointer):\n",
    "                return DPTensor(t)\n",
    "            elif isinstance(t, TensorWrappedGammaTensorPointer):\n",
    "                return DPTensor(t)\n",
    "            else:\n",
    "                assert False, f\"__torch_dispatch__ wrap got unexpected type: {type(t)}\" \n",
    "                # return t\n",
    "        def unwrap(t):\n",
    "            if isinstance(t, DPTensor):\n",
    "                return t.pointer\n",
    "            elif isinstance(t, torch.Tensor):\n",
    "                # Sometimes we get tensors (either because autograd creates tensors internally)\n",
    "                # or scalars get wrapped into tensors - wrapped number\n",
    "                return t.detach().numpy()\n",
    "            else:  # int, torch.dtype, etc? (we should have an explicit white-list)\n",
    "                return t\n",
    "            # else:\n",
    "            #     assert False, f\"Unsupported type: {type(t)}\"\n",
    "        if func in np_tensor_backend_impl:\n",
    "            rets = tree_map(wrap, np_tensor_backend_impl[func](*tree_map(unwrap, args), **tree_map(unwrap, kwargs)))\n",
    "            # get_logging_tensor_handler().emit(func.__name__, args, kwargs, rets)\n",
    "            # print('\\n'.join(get_logging_tensor_handler().log_list))\n",
    "            return rets\n",
    "        elif func in decompositions:\n",
    "            return decompositions[func](*args, **kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backend has not implemented {func.__module__}.{func.__name__}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        # TODO: maybe we should include autograd information?\n",
    "        return repr(self.pointer)\n",
    "\n",
    "    def synthetic(self):\n",
    "        return self.pointer.synthetic\n",
    "    \n",
    "    def publish(self, sigma=None):\n",
    "        ret = torch.from_numpy(self.pointer.publish(sigma=sigma).block_with_timeout(10).get()).to(self.dtype).reshape(self.shape)\n",
    "        return ret\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d286df-d58c-4af2-803c-d29723e42035",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# 4) Testing\n",
    "\n",
    "def get_op_list(tested_op_list: List[Tuple]):\n",
    "    return [opinfo for opinfo in op_db if (opinfo.name, opinfo.variant_test_name) in tested_op_list]\n",
    "\n",
    "# Some testing utils from funtorch repo (test/common_utils.py)\n",
    "\n",
    "def skipOps(test_case_name, base_test_name, to_skip):\n",
    "    all_opinfos = op_db\n",
    "    for xfail in to_skip:\n",
    "        op_name, variant_name, device_type, dtypes, expected_failure = xfail\n",
    "        matching_opinfos = [o for o in all_opinfos\n",
    "                            if o.name == op_name and o.variant_test_name == variant_name]\n",
    "        assert len(matching_opinfos) >= 1, f\"Couldn't find OpInfo for {xfail}\"\n",
    "        for opinfo in matching_opinfos:\n",
    "            decorators = list(opinfo.decorators)\n",
    "            if expected_failure:\n",
    "                decorator = DecorateInfo(unittest.expectedFailure,\n",
    "                                         test_case_name, base_test_name,\n",
    "                                         device_type=device_type, dtypes=dtypes)\n",
    "                decorators.append(decorator)\n",
    "            else:\n",
    "                decorator = DecorateInfo(unittest.skip(\"Skipped!\"),\n",
    "                                         test_case_name, base_test_name,\n",
    "                                         device_type=device_type, dtypes=dtypes)\n",
    "                decorators.append(decorator)\n",
    "            opinfo.decorators = tuple(decorators)\n",
    "\n",
    "    # This decorator doesn't modify fn in any way\n",
    "    def wrapped(fn):\n",
    "        return fn\n",
    "    return wrapped\n",
    "\n",
    "def xfail(op_name, variant_name='', *, device_type=None, dtypes=None):\n",
    "    return (op_name, variant_name, device_type, dtypes, True)\n",
    "\n",
    "_syf_owner = None\n",
    "_syf_user = None\n",
    "\n",
    "def syf_login():\n",
    "    global _syf_owner\n",
    "    global _syf_user\n",
    "\n",
    "    if _syf_owner is not None:\n",
    "        return _syf_owner, _syf_user\n",
    "\n",
    "    _syf_owner = sy.login(email=\"info@openmined.org\", password=\"changethis\", port=8081)\n",
    "    email = str(uuid1())\n",
    "    password = \"pw\"\n",
    "    if email not in [entry['email'] for entry in _syf_owner.users.all()]:\n",
    "        _syf_owner.users.create(\n",
    "            **{\n",
    "                \"name\": \"Sheldon2 Cooper\",\n",
    "                \"email\": email,\n",
    "                \"password\": password,\n",
    "                \"budget\": 1e9\n",
    "            }\n",
    "        )\n",
    "    _syf_user = sy.login(email=email, password=password, port=8081)\n",
    "    return _syf_owner, _syf_user\n",
    "\n",
    "def _allclose_with_type_promotion(a, b, rtol, atol):\n",
    "    # FIXME: float32 is not supported by Syft\n",
    "    promoted_type = torch.promote_types(a.dtype, b.dtype)\n",
    "    a = a.to(dtype=promoted_type)\n",
    "    b = b.to(dtype=promoted_type)\n",
    "    return torch.allclose(a, b, rtol, atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d4eb6-c55e-4655-a131-9d9becb365bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPTensorTest(TestCase):\n",
    "    def setUp(self):\n",
    "        owner, user = syf_login()\n",
    "        assert user.privacy_budget > 1e7\n",
    "\n",
    "    def nptensorwrapper_to_torch(self, t, sigma=1e-4):\n",
    "        # There will be noise!\n",
    "        owner, user = syf_login()\n",
    "        before = user.privacy_budget\n",
    "        ret = t.publish(sigma=sigma)\n",
    "        after = user.privacy_budget\n",
    "        assert before - after > 1\n",
    "        return ret\n",
    "\n",
    "    def wrap_samples_with_subclass(self, samples):\n",
    "        # Given OpInfo sample, reproduce sample, but with arguments wrapped\n",
    "        # Have all the sample inputs in a single data set, where each\n",
    "        # asset corresponds to a single sample\n",
    "        # Returns tuple of just \n",
    "        def handle_arg(t):\n",
    "            if isinstance(t, torch.Tensor):\n",
    "                if t.dtype == torch.float32:\n",
    "                    # FIXME: float32 is not supported by Syft\n",
    "                    t = t.to(torch.float64)\n",
    "                np_arr = t.detach().numpy()\n",
    "                assert np_arr.ndim != 0\n",
    "\n",
    "                backend_phi_tensor = sy.Tensor(np_arr)\n",
    "                data_subjects = [\"abc\"] * np_arr.shape[0] if np_arr.ndim != 0 else [\"abc\"]\n",
    "                single_data_subject = backend_phi_tensor.private(\n",
    "                    min_val=10,\n",
    "                    max_val=90,\n",
    "                    data_subjects=data_subjects)\n",
    "                return (str(uuid1()), single_data_subject)\n",
    "            else:\n",
    "                return (None, t)\n",
    "\n",
    "        owner, user = syf_login()\n",
    "        assets = {}\n",
    "        all_wrapped_args = []\n",
    "        all_wrapped_kwargs = []\n",
    "\n",
    "        for sample in samples:\n",
    "            args = [sample.input] + list(sample.args)\n",
    "            kwargs= sample.kwargs\n",
    "            wrapped_args = [handle_arg(arg) for arg in args]\n",
    "            wrapped_kwargs = {k: handle_arg(v) for k,v in kwargs.items()}\n",
    "            for k, v in wrapped_args + list(wrapped_kwargs.values()):\n",
    "                if k is not None:\n",
    "                    assets[k] = v\n",
    "            all_wrapped_args.append(wrapped_args)\n",
    "            all_wrapped_kwargs.append(wrapped_kwargs)\n",
    "\n",
    "        owner.load_dataset(\n",
    "            assets=assets,\n",
    "            name=\"my_data\",\n",
    "            description=\"description\"\n",
    "        )\n",
    "        # Shouldn't need a sleep here\n",
    "        user_assets = user.datasets[-1]\n",
    "\n",
    "        out_sample_arg_kwargs = []\n",
    "        for wrapped_args, wrapped_kwargs in zip(all_wrapped_args, all_wrapped_kwargs):\n",
    "            single_out_args = [v if k is None else DPTensor(user_assets[k]) for k, v in wrapped_args]\n",
    "            single_out_kwargs = {k: (v_v if v_k is None else DPTensor(user_assets[v_k])) for k, (v_v, v_k) in wrapped_kwargs.items()}\n",
    "            out_sample_arg_kwargs.append((single_out_args, single_out_kwargs))\n",
    "    \n",
    "        return out_sample_arg_kwargs\n",
    "\n",
    "    def torch_to_nptensorwrapper(self, t, single_data_subject=True):\n",
    "        if not single_data_subject:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            if t.dtype == torch.float32:\n",
    "                # FIXME: float32 is not supported by Syft\n",
    "                t = t.to(torch.float64)\n",
    "            np_arr = t.detach().numpy()\n",
    "            tensor_pointer = numpy_to_tensor_pointer(np_arr)\n",
    "            return DPTensor(tensor_pointer).requires_grad_(t.requires_grad)\n",
    "        else:\n",
    "            return t\n",
    "\n",
    "   # 4.1) XOR Training Example\n",
    "\n",
    "    def test_train_XOR(self):\n",
    "        import torch.nn as nn\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        SIGMA = 0.4\n",
    "        N_PER_QUAD = 100\n",
    "        N_ITER = 2\n",
    "\n",
    "        train_X_centers = [[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0]]\n",
    "        train_y = [0, 1, 1, 0]\n",
    "\n",
    "        shuffle_idx = np.arange(N_PER_QUAD * 4)\n",
    "        np.random.shuffle(shuffle_idx)\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "\n",
    "        for center, label in zip(train_X_centers, train_y):\n",
    "            train_X_list.append(np.random.randn(N_PER_QUAD, 2) * SIGMA + np.array(center))\n",
    "            train_y_list.append(np.zeros(N_PER_QUAD) if label == 0 else np.ones(N_PER_QUAD))\n",
    "\n",
    "        # Linear layer does not support double\n",
    "        train_X_np = np.concatenate(train_X_list)[shuffle_idx].astype(np.float64)\n",
    "        train_y_np = np.concatenate(train_y_list)[shuffle_idx].astype(np.float64)\n",
    "\n",
    "        # Convert input data to torch.Tensor\n",
    "        train_X = DPTensor(numpy_to_tensor_pointer(train_X_np))\n",
    "        train_y = DPTensor(numpy_to_tensor_pointer(train_y_np))\n",
    "\n",
    "        # Simple MLP\n",
    "\n",
    "        class Net(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Net, self).__init__()\n",
    "                self.linear1 = nn.Linear(2, 10)\n",
    "                self.linear2 = nn.Linear(10, 1)\n",
    "                self.relu = nn.ReLU()\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.linear1(x)\n",
    "                x = self.relu(x)\n",
    "                x = self.linear2(x)\n",
    "                x = self.relu(x)\n",
    "                return x.sigmoid()\n",
    "\n",
    "        # Train\n",
    "\n",
    "        model =  Net()\n",
    "        sgd = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "        # Register hook that publishes gradients (unwraps)\n",
    "        for p in model.parameters():\n",
    "            p.register_hook(lambda x: x.publish(sigma=1e-4))\n",
    "\n",
    "        # For testing only\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        train_X_tensor = torch.from_numpy(train_X_np).to(torch.float32)\n",
    "        train_y_tensor = torch.from_numpy(train_y_np).to(torch.float32)\n",
    "        old_params = [p.clone().detach() for p in model.parameters()]\n",
    "        losses = []\n",
    "\n",
    "        # Training loop\n",
    "        for i in range(N_ITER):\n",
    "            print(\"ITER: \", i)\n",
    "            out = model(train_X)\n",
    "            loss = (out - train_y).sum()\n",
    "            loss.backward()\n",
    "\n",
    "            # Testing computed gradients are the same\n",
    "            out1 = model_copy(train_X_tensor)\n",
    "            loss1 = (out1 - train_y_tensor).sum()\n",
    "            loss1.backward()\n",
    "            if i == 0:\n",
    "                for p1, p2 in zip(model.parameters(), model_copy.parameters()):\n",
    "                    assert torch.allclose(p1.grad, p2.grad, atol=3e-2, rtol=3e-2)\n",
    "\n",
    "            sgd.step()\n",
    "            sgd.zero_grad()\n",
    "            losses.append(loss.detach())\n",
    "\n",
    "        # Test parameters have been updated\n",
    "        for p1, p2 in zip(old_params, model.parameters()):\n",
    "            assert not torch.allclose(p1, p2)\n",
    "\n",
    "        # Run test set\n",
    "\n",
    "        test_X = torch.tensor([[-1., -1.], [-1., 1.], [1., -1.], [1., 1.]])\n",
    "        test_y = torch.tensor([[0., 1., 1., 0.]])\n",
    "        predicted = model(test_X)\n",
    "\n",
    "        # Check whether we predictions are good\n",
    "        # FIXME: currently runs too slow (need about 200 iter to reliably predict)\n",
    "\n",
    "        # assert torch.allclose(predicted, test_y, atol=1e-2)\n",
    "\n",
    "\n",
    "    # 4.2) Op Testing\n",
    "\n",
    "    @ops(op_list=get_op_list([\n",
    "        # Backend ops\n",
    "        # ('sum',''),  # int dim list?\n",
    "        ('add',''),\n",
    "        ('sub',''),\n",
    "        # ('rsub',''),\n",
    "        ('div',''),\n",
    "        ('mul',''),\n",
    "        ('exp',''),\n",
    "        # Decompositions\n",
    "        # ('softmax',''),\n",
    "        # ('nn.functional.mse_loss',''),\n",
    "        # ('nn.functional.l1_loss',''),\n",
    "    ]), allowed_dtypes=(torch.float,))\n",
    "    @skipOps('NPTensorTest', 'test_np_tensor_parity', {\n",
    "        # Need to accept `alpha` param\n",
    "        xfail('add'),\n",
    "        xfail('sub'),\n",
    "        xfail('exp'),  # FIXME: This is weird - result off by 20 even when sigma is 1e-4!\n",
    "        # We didn't implement a specific overload\n",
    "        xfail('rsub',''),  # need: rsub.Tensor\n",
    "    })\n",
    "    def test_np_tensor_parity(self, device, dtype, op):\n",
    "        # TODO: faster way to run tests, load all datasets at the same time?\n",
    "        # Tests if tensor and subclass tensor compute the same values\n",
    "        assert device == 'cpu' and  dtype == torch.float\n",
    "\n",
    "        samples = list(op.sample_inputs(device, dtype, requires_grad=False))\n",
    "\n",
    "        def supported_by_syft(sample):\n",
    "            element_wise_binary_ops = \"add sub rsub div mul\".split(\" \")\n",
    "            arg_values = [sample.input] + list(sample.args)\n",
    "            kwarg_values = sample.kwargs\n",
    "            if any(t.numel() == 0 for t in arg_values if isinstance(t, torch.Tensor)) or \\\n",
    "                any(t.numel() == 0 for t in kwarg_values.values() if isinstance(t, torch.Tensor)):\n",
    "                # FIXME: zero-numel tensor errors (data subject issue?)\n",
    "                return False\n",
    "            if any(t.ndim == 0 for t in arg_values if isinstance(t, torch.Tensor)) or \\\n",
    "                any(t.ndim == 0 for t in kwarg_values.values() if isinstance(t, torch.Tensor)):\n",
    "                # FIXME: Scalar errors (data subject issue?)\n",
    "                return False\n",
    "            if op.name in element_wise_binary_ops and arg_values[0].shape[0] != arg_values[1].shape[0]:\n",
    "                # FIXME: element-wise op when data_subjects_indexed arrays are differently shaped\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        samples = list(filter(supported_by_syft, samples))\n",
    "        wrapped_samples = self.wrap_samples_with_subclass(samples)\n",
    "        for wrapped_sample, sample in zip(wrapped_samples, samples):\n",
    "            # print(\"Sample: \", sample)\n",
    "            # print(\"Wrapped sample: \", wrapped_sample)\n",
    "            # Wrapped\n",
    "            wrapped_args, wrapped_kwargs = wrapped_sample\n",
    "            fn = op.get_op()\n",
    "            raw_output = fn(*wrapped_args, **wrapped_kwargs)\n",
    "            result = self.nptensorwrapper_to_torch(raw_output)\n",
    "\n",
    "            # Non-Wrapped\n",
    "            arg_values = [sample.input] + list(sample.args)\n",
    "            kwarg_values = sample.kwargs\n",
    "            expected = fn(*arg_values, **kwarg_values)\n",
    "            print(result, expected, (result - expected).abs().max())\n",
    "            # print(arg_values)\n",
    "\n",
    "            self.assertTrue(_allclose_with_type_promotion(result, expected, rtol=1e-1, atol=1e-2))\n",
    "\n",
    "    # @ops(op_list=get_op_list([\n",
    "    #     # Backend ops\n",
    "    #     ('sum',''),\n",
    "    #     ('add',''),\n",
    "    #     ('sub',''),\n",
    "    #     ('div',''),\n",
    "    #     ('mul',''),\n",
    "    #     # Decompositions\n",
    "    #     ('sigmoid',''),\n",
    "    #     ('nn.functional.mse_loss',''),\n",
    "    # ]), allowed_dtypes=(torch.float,))\n",
    "    # @skipOps('NPTensorTest', 'test_np_tensor_gradients_parity', {\n",
    "    #     # Forward is already failing for these\n",
    "    #     xfail('add'),\n",
    "    #     xfail('sub'),\n",
    "    #     xfail('rsub',''),\n",
    "    # })\n",
    "    # def test_np_tensor_gradients_parity(self, device, dtype, op):\n",
    "    #     # Tests if tensor and subclass tensor compute the same gradient\n",
    "    #     assert device == 'cpu' and dtype == torch.float\n",
    "\n",
    "    #     samples = op.sample_inputs(device, dtype, requires_grad=True)\n",
    "\n",
    "    #     for sample in samples:\n",
    "    #         arg_values = [sample.input] + list(sample.args)\n",
    "    #         kwarg_values = sample.kwargs\n",
    "\n",
    "    #         subclass_arg_values = tree_map(self.torch_to_nptensorwrapper, arg_values)\n",
    "    #         subclass_kwarg_values = tree_map(self.torch_to_nptensorwrapper, kwarg_values)\n",
    "\n",
    "    #         fn = op.get_op()\n",
    "\n",
    "    #         subclass_out = fn(*subclass_arg_values, **subclass_kwarg_values)\n",
    "    #         out = fn(*arg_values, **kwarg_values)\n",
    "\n",
    "    #         assert isinstance(subclass_out, torch.Tensor), f\"Expected op to return a single tensor, but got: {type(subclass_out)}\"\n",
    "\n",
    "    #         grad_outputs = torch.rand_like(out)\n",
    "    #         subclass_grad_inputs = torch.autograd.grad(subclass_out, subclass_arg_values, grad_outputs=grad_outputs)\n",
    "    #         subclass_grad_inputs = tree_map(self.nptensorwrapper_to_torch, subclass_grad_inputs)\n",
    "\n",
    "    #         grad_inputs = torch.autograd.grad(out, arg_values, grad_outputs=grad_outputs)\n",
    "\n",
    "    #         self.assertEqual(len(subclass_grad_inputs), len(grad_inputs))\n",
    "\n",
    "    #         for subclass_grad_input, grad_input in zip(subclass_grad_inputs, grad_inputs):\n",
    "    #             # print(subclass_grad_input, grad_input, (subclass_grad_input - grad_input).abs().max())\n",
    "    #             # print(arg_values)\n",
    "    #             self.assertTrue(torch.allclose(grad_input, subclass_grad_input))\n",
    "\n",
    "instantiate_device_type_tests(NPTensorTest, globals(), only_for=(\"cpu,\"))\n",
    "\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a252b77-4d8b-4fff-aa6b-c46c5920bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ea6932-e603-483b-be8a-d88bb7970348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CHANGE YOUR USERNAME AND PASSWORD!!! \n",
      "\n",
      "Anyone can login as an admin to your node right now because your password is still the default PySyft username and password!!!\n",
      "\n",
      "Connecting to localhost... done! \t Logging into canada... done!\n"
     ]
    }
   ],
   "source": [
    "canada= sy.login(email = \"info@openmined.org\",password=\"changethis\",port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a228cb5-4819-4138-8aeb-edabc5fdaf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a66e5d-9f05-421c-a38b-f61b90d4ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = sy.Tensor(data).private(min_val=0,max_val = 1,data_subjects=[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8d9fa1-9dc2-4c2d-b7a5-4c70a7419c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kj/filesystem-disk-unix.c++:1690: warning: PWD environment variable doesn't match current directory; pwd = /home/azureuser/PySyft\n",
      "Uploading `36f2754c817440c089753da67c403eb2`: 100%|\u001b[32m██████████████████\u001b[0m| 1/1 [00:00<00:00, 256.33it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ptr = data.send(canada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297cb572-2df6-42ae-ad4a-4c728486b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = ptr.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7cba51-4baf-4a59-a163-5f99d72dcfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading `da3c83dff087481ca02e6f0977178ae1`: 100%|\u001b[32m██████████████████\u001b[0m| 1/1 [00:00<00:00, 264.94it/s]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(child=GammaTensor(child=array([2.71828183]), data_subjects=<syft.core.adp.data_subject_list.DataSubjectList object at 0x7f5524b20a30>, min_val=<lazyrepeatarray data: [1.] -> shape: (1,)>, max_val=<lazyrepeatarray data: [2.71828183] -> shape: (1,)>, is_linear=True, func=<function no_op at 0x7f5525dec310>, id='96117006', state={'1714072715': GammaTensor(child=array([1]), data_subjects=<syft.core.adp.data_subject_list.DataSubjectList object at 0x7f5524b202e0>, min_val=<lazyrepeatarray data: [0] -> shape: (1,)>, max_val=<lazyrepeatarray data: [1] -> shape: (1,)>, is_linear=True, func=<function no_op at 0x7f5525dec310>, id='1714072715', state={})}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.gamma.send(canada).exp().block.get_copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
