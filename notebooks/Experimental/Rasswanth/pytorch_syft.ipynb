{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e7bbe-dd24-4dcc-a4ab-eaf3a9abf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "import unittest\n",
    "from uuid import uuid1\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch \n",
    "from torch.testing._internal.common_utils import TestCase, run_tests\n",
    "\n",
    "numpy_to_torch_dtype_dict = {\n",
    "    np.bool_      : torch.bool,\n",
    "    np.uint8      : torch.uint8,\n",
    "    np.int8       : torch.int8,\n",
    "    np.int16      : torch.int16,\n",
    "    np.int32      : torch.int32,\n",
    "    np.int64      : torch.int64,\n",
    "    np.float16    : torch.float16,\n",
    "    np.float32    : torch.float32,\n",
    "    np.float64    : torch.float32,  # float64 -> float32\n",
    "    np.complex64  : torch.complex64,\n",
    "    np.complex128 : torch.complex128\n",
    "}\n",
    "\n",
    "from torch.testing._internal.common_device_type import ops, instantiate_device_type_tests\n",
    "from torch.testing._internal.common_methods_invocations import op_db, DecorateInfo\n",
    "from torch.utils._pytree import tree_map\n",
    "\n",
    "import syft as sy\n",
    "from syft.core.tensor.autodp.phi_tensor import TensorWrappedPhiTensorPointer, TensorWrappedGammaTensorPointer\n",
    "sy.logger.remove()\n",
    "\n",
    "aten = torch.ops.aten\n",
    "\n",
    "np_tensor_backend_impl = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892dba45-d56a-48e0-a9f3-40849d7675cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_tensor_pointer(np_arr):\n",
    "    owner, user = syf_login()\n",
    "    backend_phi_tensor = sy.Tensor(np_arr)\n",
    "    assert np_arr.ndim != 0\n",
    "    data_subjects = [\"abc\"] * np_arr.shape[0] if np_arr.ndim != 0 else [\"abc\"]\n",
    "    single_data_subject = backend_phi_tensor.private(\n",
    "        min_val=10,\n",
    "        max_val=90,\n",
    "        data_subjects=data_subjects)\n",
    "    data_asset_name = str(uuid1())\n",
    "    owner.load_dataset(\n",
    "        assets={\n",
    "            data_asset_name: single_data_subject,\n",
    "        },\n",
    "        name=\"my_data\",\n",
    "        description=\"description\"\n",
    "    )\n",
    "    tensor_pointer = user.datasets[-1][data_asset_name]\n",
    "    assert isinstance(tensor_pointer, TensorWrappedPhiTensorPointer)\n",
    "    return tensor_pointer\n",
    "\n",
    "def register_func(name, ns):\n",
    "    # Helpful decorator to colocate registration and definition\n",
    "    def register_func_inner(f):\n",
    "        assert name not in ns, f\"{name} has already been registered\"\n",
    "        ns[name] = f\n",
    "        return f\n",
    "    return register_func_inner\n",
    "\n",
    "def torch_to_numpy_dtype_dict_int(dtype_int):\n",
    "    # FIXME: Looks like this has been fixed by core, I don't think we need this anymore\n",
    "    if dtype_int == torch.float32:\n",
    "        return np.float64\n",
    "    else:\n",
    "        assert False, f\"got: {dtype_int=}\"\n",
    "\n",
    "def numpy_to_torch(t):\n",
    "    # Replacement for torch.from_numpy that actually handles scalars\n",
    "    if np.isscalar(t):\n",
    "        return torch.tensor(t)\n",
    "    else:\n",
    "        return torch.from_numpy(t)\n",
    "\n",
    "@register_func(aten.add.Tensor, np_tensor_backend_impl)\n",
    "def np_tensor_add_impl(x, y):  # @fix_binary_type_promotion\n",
    "    return x + y\n",
    "\n",
    "@register_func(aten.sub.Tensor, np_tensor_backend_impl)\n",
    "def np_tensor_sub_impl(x, y):  # @fix_binary_type_promotion\n",
    "    return x - y\n",
    "\n",
    "@register_func(aten.rsub.Scalar, np_tensor_backend_impl)\n",
    "def np_tensor_rsub_impl(x, y):  # @fix_binary_type_promotion\n",
    "    return y - x\n",
    "\n",
    "@register_func(aten.mul.Tensor, np_tensor_backend_impl)\n",
    "def np_tensor_mul_impl(x, y):  # @fix_binary_type_promotion\n",
    "    return x * y\n",
    "\n",
    "@register_func(aten.div.Tensor, np_tensor_backend_impl)\n",
    "def np_tensor_div_impl(x, y):  # @fix_binary_type_promotion\n",
    "    return x / y\n",
    "\n",
    "@register_func(aten.sum.default, np_tensor_backend_impl)\n",
    "def np_tensor_sum_default_impl(x, keepdims=False):\n",
    "    assert not keepdims\n",
    "    return x.sum()\n",
    "\n",
    "@register_func(aten.sum.dim_IntList, np_tensor_backend_impl)\n",
    "def np_tensor_sum_dim_IntList_impl(x, dims, keepdims=False):\n",
    "    dims = (dims,) if isinstance(dims, int) else tuple(dims)\n",
    "    if len(dims) == 1 and x.public_shape[dims[0]] == 1 and keepdims:\n",
    "        return x + 0\n",
    "    np_tensor = x.publish(sigma=1e-4).block_with_timeout(10).get()\n",
    "    return numpy_to_tensor_pointer(np.sum(np_tensor, axis=dims, keepdims=keepdims))\n",
    "\n",
    "@register_func(aten.gt.Scalar, np_tensor_backend_impl)\n",
    "def np_tensor_gt_impl(x, y):\n",
    "    return x > y\n",
    "\n",
    "@register_func(aten.reciprocal.default, np_tensor_backend_impl)\n",
    "def np_tensor_reciprocal_impl(x):\n",
    "    return x.reciprocal()\n",
    "\n",
    "@register_func(aten.exp.default, np_tensor_backend_impl)\n",
    "def np_tensor_exp_impl(x):\n",
    "    return x.exp()\n",
    "\n",
    "@register_func(aten.mm.default, np_tensor_backend_impl)\n",
    "def np_tensor_mm_impl(x, y):\n",
    "    ret = x @ y\n",
    "    return ret\n",
    "\n",
    "# View Ops\n",
    "\n",
    "@register_func(aten.view.default, np_tensor_backend_impl)\n",
    "def np_tensor_view_impl(x, shape):\n",
    "    if x.public_shape == shape:\n",
    "        return x\n",
    "    np_tensor = x.publish(sigma=1e-4).block_with_timeout(10).get()\n",
    "    ret = np_tensor.view()\n",
    "    ret.shape = shape\n",
    "    return numpy_to_tensor_pointer(ret)\n",
    "\n",
    "@register_func(aten.expand.default, np_tensor_backend_impl)\n",
    "def np_tensor_expand_impl(x, out_size):\n",
    "    # FIXME: we don't have views yet, so just make a new tensor for now\n",
    "    np_tensor = x.publish(sigma=1e-4).block_with_timeout(10).get()\n",
    "    return numpy_to_tensor_pointer(np.broadcast_to(np_tensor, out_size))\n",
    "\n",
    "@register_func(aten.t.default, np_tensor_backend_impl)\n",
    "def np_tensor_t_impl(x):\n",
    "    # Question: This is a view?\n",
    "    return x.T\n",
    "\n",
    "@register_func(aten.detach.default, np_tensor_backend_impl)\n",
    "def np_tensor_detach_impl(x):\n",
    "    # To the backend, detach is simply a view (FIXME: but we don't have views yet\n",
    "    # so, just do a \"clone\" because the shape is the same)\n",
    "    return x + 0\n",
    "\n",
    "# Casting ops\n",
    "\n",
    "@register_func(aten._to_copy.default, np_tensor_backend_impl)\n",
    "def np_tensor__to_copy_impl(x, dtype):\n",
    "    return x + 0  # FIXME: cast -> clone\n",
    "\n",
    "# Factory ops\n",
    "\n",
    "@register_func(aten.ones_like.default, np_tensor_backend_impl)\n",
    "def np_tensor_ones_like_impl(x, dtype, layout, device, pin_memory, memory_format):\n",
    "    # FIXME: factory functions - just make a new tensor? (this should be handled by backend)\n",
    "    np_tensor = x.publish(sigma=1e-4).block_with_timeout(10).get()\n",
    "    if len(np_tensor.shape) == 0:\n",
    "        # This is weird, public shape is not a scalar, but after publishing we get a scalar\n",
    "        np_tensor = np.broadcast_to(np_tensor, (1,))\n",
    "    return numpy_to_tensor_pointer(np.ones_like(np_tensor, dtype=torch_to_numpy_dtype_dict_int(dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184e6a6-7745-4f78-85e6-d77da46c207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decompositions = dict()\n",
    "\n",
    "@register_func(aten.relu.default, decompositions)\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "@register_func(aten.addmm.default, decompositions)\n",
    "def addmm(bias, a, b):\n",
    "    return torch.mm(a, b) + bias\n",
    "\n",
    "@register_func(aten.sigmoid.default, decompositions)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(x * -1))\n",
    "\n",
    "@register_func(aten.sigmoid_backward.default, decompositions)\n",
    "def sigmoid_backward(grad_out, result):\n",
    "    # FIXME: DPTensor needs to be on the LHS always?\n",
    "    return grad_out * result * (result * -1 + 1)\n",
    "\n",
    "@register_func(aten.threshold_backward.default, decompositions)\n",
    "def threshold_backward(grad_out, self, threshold):\n",
    "    return grad_out * (self > threshold)\n",
    "\n",
    "def contig_strides_from_shape(shape):\n",
    "    return torch.zeros(shape).stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b6059-f3f1-438c-97ad-568ff892a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPTensor(torch.Tensor):\n",
    "    # This class wraps a TensorPointer so autograd, and other can be used\n",
    "    #\n",
    "    # TODO:\n",
    "    # - Think about the gamma case\n",
    "    # - nit: maybe use slots here to be more efficient\n",
    "    @staticmethod\n",
    "    def __new__(cls, pointer, requires_grad=False):\n",
    "        assert isinstance(pointer, TensorWrappedPhiTensorPointer) or \\\n",
    "            isinstance(pointer, TensorWrappedGammaTensorPointer)\n",
    "        r = torch.Tensor._make_wrapper_subclass(  # type: ignore[attr-defined]\n",
    "            cls,\n",
    "            pointer.public_shape,\n",
    "            strides=contig_strides_from_shape(pointer.public_shape),\n",
    "            storage_offset=0,  # elem.storage_offset(),\n",
    "            # TODO: clone storage aliasing\n",
    "            # FIXME: why don't we just return a np type instead of a string?\n",
    "            dtype=numpy_to_torch_dtype_dict[getattr(np, pointer.public_dtype)],\n",
    "            layout=torch.strided,\n",
    "            device=\"cpu\",  # elem.device\n",
    "            requires_grad=requires_grad\n",
    "        )\n",
    "        r.pointer = pointer\n",
    "\n",
    "        return r\n",
    "        \n",
    "    __torch_function__ = torch._C._disabled_torch_function_impl\n",
    "    \n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "        print(f\"__torch_dispatch__: {func.__module__}.{func.__name__}\")\n",
    "        def wrap(t):\n",
    "            if isinstance(t, TensorWrappedPhiTensorPointer):\n",
    "                return DPTensor(t)\n",
    "            elif isinstance(t, TensorWrappedGammaTensorPointer):\n",
    "                return DPTensor(t)\n",
    "            else:\n",
    "                assert False, f\"__torch_dispatch__ wrap got unexpected type: {type(t)}\" \n",
    "                # return t\n",
    "        def unwrap(t):\n",
    "            if isinstance(t, DPTensor):\n",
    "                return t.pointer\n",
    "            elif isinstance(t, torch.Tensor):\n",
    "                # Sometimes we get tensors (either because autograd creates tensors internally)\n",
    "                # or scalars get wrapped into tensors - wrapped number\n",
    "                return t.detach().numpy()\n",
    "            else:  # int, torch.dtype, etc? (we should have an explicit white-list)\n",
    "                return t\n",
    "            # else:\n",
    "            #     assert False, f\"Unsupported type: {type(t)}\"\n",
    "        if func in np_tensor_backend_impl:\n",
    "            rets = tree_map(wrap, np_tensor_backend_impl[func](*tree_map(unwrap, args), **tree_map(unwrap, kwargs)))\n",
    "            # get_logging_tensor_handler().emit(func.__name__, args, kwargs, rets)\n",
    "            # print('\\n'.join(get_logging_tensor_handler().log_list))\n",
    "            return rets\n",
    "        elif func in decompositions:\n",
    "            return decompositions[func](*args, **kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backend has not implemented {func.__module__}.{func.__name__}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        # TODO: maybe we should include autograd information?\n",
    "        return repr(self.pointer)\n",
    "\n",
    "    def synthetic(self):\n",
    "        return self.pointer.synthetic\n",
    "    \n",
    "    def publish(self, sigma=None):\n",
    "        ret = torch.from_numpy(self.pointer.publish(sigma=sigma).block_with_timeout(10).get()).to(self.dtype).reshape(self.shape)\n",
    "        return ret\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aea181-6420-4f9b-9a10-095f44603b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# 4) Testing\n",
    "\n",
    "def get_op_list(tested_op_list: List[Tuple]):\n",
    "    return [opinfo for opinfo in op_db if (opinfo.name, opinfo.variant_test_name) in tested_op_list]\n",
    "\n",
    "# Some testing utils from funtorch repo (test/common_utils.py)\n",
    "\n",
    "def skipOps(test_case_name, base_test_name, to_skip):\n",
    "    all_opinfos = op_db\n",
    "    for xfail in to_skip:\n",
    "        op_name, variant_name, device_type, dtypes, expected_failure = xfail\n",
    "        matching_opinfos = [o for o in all_opinfos\n",
    "                            if o.name == op_name and o.variant_test_name == variant_name]\n",
    "        assert len(matching_opinfos) >= 1, f\"Couldn't find OpInfo for {xfail}\"\n",
    "        for opinfo in matching_opinfos:\n",
    "            decorators = list(opinfo.decorators)\n",
    "            if expected_failure:\n",
    "                decorator = DecorateInfo(unittest.expectedFailure,\n",
    "                                         test_case_name, base_test_name,\n",
    "                                         device_type=device_type, dtypes=dtypes)\n",
    "                decorators.append(decorator)\n",
    "            else:\n",
    "                decorator = DecorateInfo(unittest.skip(\"Skipped!\"),\n",
    "                                         test_case_name, base_test_name,\n",
    "                                         device_type=device_type, dtypes=dtypes)\n",
    "                decorators.append(decorator)\n",
    "            opinfo.decorators = tuple(decorators)\n",
    "\n",
    "    # This decorator doesn't modify fn in any way\n",
    "    def wrapped(fn):\n",
    "        return fn\n",
    "    return wrapped\n",
    "\n",
    "def xfail(op_name, variant_name='', *, device_type=None, dtypes=None):\n",
    "    return (op_name, variant_name, device_type, dtypes, True)\n",
    "\n",
    "_syf_owner = None\n",
    "_syf_user = None\n",
    "\n",
    "def syf_login():\n",
    "    global _syf_owner\n",
    "    global _syf_user\n",
    "\n",
    "    if _syf_owner is not None:\n",
    "        return _syf_owner, _syf_user\n",
    "\n",
    "    _syf_owner = sy.login(email=\"info@openmined.org\", password=\"changethis\", port=8081)\n",
    "    email = str(uuid1())\n",
    "    password = \"pw\"\n",
    "    if email not in [entry['email'] for entry in _syf_owner.users.all()]:\n",
    "        _syf_owner.users.create(\n",
    "            **{\n",
    "                \"name\": \"Sheldon2 Cooper\",\n",
    "                \"email\": email,\n",
    "                \"password\": password,\n",
    "                \"budget\": 1e9\n",
    "            }\n",
    "        )\n",
    "    _syf_user = sy.login(email=email, password=password, port=8081)\n",
    "    return _syf_owner, _syf_user\n",
    "\n",
    "def _allclose_with_type_promotion(a, b, rtol, atol):\n",
    "    # FIXME: float32 is not supported by Syft\n",
    "    promoted_type = torch.promote_types(a.dtype, b.dtype)\n",
    "    a = a.to(dtype=promoted_type)\n",
    "    b = b.to(dtype=promoted_type)\n",
    "    return torch.allclose(a, b, rtol, atol)\n",
    "\n",
    "class NPTensorTest(TestCase):\n",
    "    def setUp(self):\n",
    "        owner, user = syf_login()\n",
    "        assert user.privacy_budget > 1e7\n",
    "\n",
    "    def nptensorwrapper_to_torch(self, t, sigma=1e-4):\n",
    "        # There will be noise!\n",
    "        owner, user = syf_login()\n",
    "        before = user.privacy_budget\n",
    "        ret = t.publish(sigma=sigma)\n",
    "        after = user.privacy_budget\n",
    "        assert before - after > 1\n",
    "        return ret\n",
    "\n",
    "    def wrap_samples_with_subclass(self, samples):\n",
    "        # Given OpInfo sample, reproduce sample, but with arguments wrapped\n",
    "        # Have all the sample inputs in a single data set, where each\n",
    "        # asset corresponds to a single sample\n",
    "        # Returns tuple of just \n",
    "        def handle_arg(t):\n",
    "            if isinstance(t, torch.Tensor):\n",
    "                if t.dtype == torch.float32:\n",
    "                    # FIXME: float32 is not supported by Syft\n",
    "                    t = t.to(torch.float64)\n",
    "                np_arr = t.detach().numpy()\n",
    "                assert np_arr.ndim != 0\n",
    "\n",
    "                backend_phi_tensor = sy.Tensor(np_arr)\n",
    "                data_subjects = [\"abc\"] * np_arr.shape[0] if np_arr.ndim != 0 else [\"abc\"]\n",
    "                single_data_subject = backend_phi_tensor.private(\n",
    "                    min_val=10,\n",
    "                    max_val=90,\n",
    "                    data_subjects=data_subjects)\n",
    "                return (str(uuid1()), single_data_subject)\n",
    "            else:\n",
    "                return (None, t)\n",
    "\n",
    "        owner, user = syf_login()\n",
    "        assets = {}\n",
    "        all_wrapped_args = []\n",
    "        all_wrapped_kwargs = []\n",
    "\n",
    "        for sample in samples:\n",
    "            args = [sample.input] + list(sample.args)\n",
    "            kwargs= sample.kwargs\n",
    "            wrapped_args = [handle_arg(arg) for arg in args]\n",
    "            wrapped_kwargs = {k: handle_arg(v) for k,v in kwargs.items()}\n",
    "            for k, v in wrapped_args + list(wrapped_kwargs.values()):\n",
    "                if k is not None:\n",
    "                    assets[k] = v\n",
    "            all_wrapped_args.append(wrapped_args)\n",
    "            all_wrapped_kwargs.append(wrapped_kwargs)\n",
    "\n",
    "        owner.load_dataset(\n",
    "            assets=assets,\n",
    "            name=\"my_data\",\n",
    "            description=\"description\"\n",
    "        )\n",
    "        # Shouldn't need a sleep here\n",
    "        user_assets = user.datasets[-1]\n",
    "\n",
    "        out_sample_arg_kwargs = []\n",
    "        for wrapped_args, wrapped_kwargs in zip(all_wrapped_args, all_wrapped_kwargs):\n",
    "            single_out_args = [v if k is None else DPTensor(user_assets[k]) for k, v in wrapped_args]\n",
    "            single_out_kwargs = {k: (v_v if v_k is None else DPTensor(user_assets[v_k])) for k, (v_v, v_k) in wrapped_kwargs.items()}\n",
    "            out_sample_arg_kwargs.append((single_out_args, single_out_kwargs))\n",
    "    \n",
    "        return out_sample_arg_kwargs\n",
    "\n",
    "    def torch_to_nptensorwrapper(self, t, single_data_subject=True):\n",
    "        if not single_data_subject:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            if t.dtype == torch.float32:\n",
    "                # FIXME: float32 is not supported by Syft\n",
    "                t = t.to(torch.float64)\n",
    "            np_arr = t.detach().numpy()\n",
    "            tensor_pointer = numpy_to_tensor_pointer(np_arr)\n",
    "            return DPTensor(tensor_pointer).requires_grad_(t.requires_grad)\n",
    "        else:\n",
    "            return t\n",
    "\n",
    "   # 4.1) XOR Training Example\n",
    "\n",
    "    def test_train_XOR(self):\n",
    "        import torch.nn as nn\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        SIGMA = 0.4\n",
    "        N_PER_QUAD = 100\n",
    "        N_ITER = 2\n",
    "\n",
    "        train_X_centers = [[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0]]\n",
    "        train_y = [0, 1, 1, 0]\n",
    "\n",
    "        shuffle_idx = np.arange(N_PER_QUAD * 4)\n",
    "        np.random.shuffle(shuffle_idx)\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "\n",
    "        for center, label in zip(train_X_centers, train_y):\n",
    "            train_X_list.append(np.random.randn(N_PER_QUAD, 2) * SIGMA + np.array(center))\n",
    "            train_y_list.append(np.zeros(N_PER_QUAD) if label == 0 else np.ones(N_PER_QUAD))\n",
    "\n",
    "        # Linear layer does not support double\n",
    "        train_X_np = np.concatenate(train_X_list)[shuffle_idx].astype(np.float64)\n",
    "        train_y_np = np.concatenate(train_y_list)[shuffle_idx].astype(np.float64)\n",
    "\n",
    "        # Convert input data to torch.Tensor\n",
    "        train_X = DPTensor(numpy_to_tensor_pointer(train_X_np))\n",
    "        train_y = DPTensor(numpy_to_tensor_pointer(train_y_np))\n",
    "\n",
    "        # Simple MLP\n",
    "\n",
    "        class Net(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Net, self).__init__()\n",
    "                self.linear1 = nn.Linear(2, 10)\n",
    "                self.linear2 = nn.Linear(10, 1)\n",
    "                self.relu = nn.ReLU()\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.linear1(x)\n",
    "                x = self.relu(x)\n",
    "                x = self.linear2(x)\n",
    "                x = self.relu(x)\n",
    "                return x.sigmoid()\n",
    "\n",
    "        # Train\n",
    "\n",
    "        model =  Net()\n",
    "        sgd = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "        # Register hook that publishes gradients (unwraps)\n",
    "        for p in model.parameters():\n",
    "            p.register_hook(lambda x: x.publish(sigma=1e-4))\n",
    "\n",
    "        # For testing only\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        train_X_tensor = torch.from_numpy(train_X_np).to(torch.float32)\n",
    "        train_y_tensor = torch.from_numpy(train_y_np).to(torch.float32)\n",
    "        old_params = [p.clone().detach() for p in model.parameters()]\n",
    "        losses = []\n",
    "\n",
    "        # Training loop\n",
    "        for i in range(N_ITER):\n",
    "            print(\"ITER: \", i)\n",
    "            out = model(train_X)\n",
    "            loss = (out - train_y).sum()\n",
    "            loss.backward()\n",
    "\n",
    "            # Testing computed gradients are the same\n",
    "            out1 = model_copy(train_X_tensor)\n",
    "            loss1 = (out1 - train_y_tensor).sum()\n",
    "            loss1.backward()\n",
    "            if i == 0:\n",
    "                for p1, p2 in zip(model.parameters(), model_copy.parameters()):\n",
    "                    assert torch.allclose(p1.grad, p2.grad, atol=3e-2, rtol=3e-2)\n",
    "\n",
    "            sgd.step()\n",
    "            sgd.zero_grad()\n",
    "            losses.append(loss.detach())\n",
    "\n",
    "        # Test parameters have been updated\n",
    "        for p1, p2 in zip(old_params, model.parameters()):\n",
    "            assert not torch.allclose(p1, p2)\n",
    "\n",
    "        # Run test set\n",
    "\n",
    "        test_X = torch.tensor([[-1., -1.], [-1., 1.], [1., -1.], [1., 1.]])\n",
    "        test_y = torch.tensor([[0., 1., 1., 0.]])\n",
    "        predicted = model(test_X)\n",
    "\n",
    "        # Check whether we predictions are good\n",
    "        # FIXME: currently runs too slow (need about 200 iter to reliably predict)\n",
    "\n",
    "        # assert torch.allclose(predicted, test_y, atol=1e-2)\n",
    "\n",
    "\n",
    "    # 4.2) Op Testing\n",
    "\n",
    "    @ops(op_list=get_op_list([\n",
    "        # Backend ops\n",
    "        # ('sum',''),  # int dim list?\n",
    "        ('add',''),\n",
    "        ('sub',''),\n",
    "        # ('rsub',''),\n",
    "        ('div',''),\n",
    "        ('mul',''),\n",
    "        ('exp',''),\n",
    "        # Decompositions\n",
    "        # ('softmax',''),\n",
    "        # ('nn.functional.mse_loss',''),\n",
    "        # ('nn.functional.l1_loss',''),\n",
    "    ]), allowed_dtypes=(torch.float,))\n",
    "    @skipOps('NPTensorTest', 'test_np_tensor_parity', {\n",
    "        # Need to accept `alpha` param\n",
    "        xfail('add'),\n",
    "        xfail('sub'),\n",
    "        xfail('exp'),  # FIXME: This is weird - result off by 20 even when sigma is 1e-4!\n",
    "        # We didn't implement a specific overload\n",
    "        xfail('rsub',''),  # need: rsub.Tensor\n",
    "    })\n",
    "    def test_np_tensor_parity(self, device, dtype, op):\n",
    "        # TODO: faster way to run tests, load all datasets at the same time?\n",
    "        # Tests if tensor and subclass tensor compute the same values\n",
    "        assert device == 'cpu' and  dtype == torch.float\n",
    "\n",
    "        samples = list(op.sample_inputs(device, dtype, requires_grad=False))\n",
    "\n",
    "        def supported_by_syft(sample):\n",
    "            element_wise_binary_ops = \"add sub rsub div mul\".split(\" \")\n",
    "            arg_values = [sample.input] + list(sample.args)\n",
    "            kwarg_values = sample.kwargs\n",
    "            if any(t.numel() == 0 for t in arg_values if isinstance(t, torch.Tensor)) or \\\n",
    "                any(t.numel() == 0 for t in kwarg_values.values() if isinstance(t, torch.Tensor)):\n",
    "                # FIXME: zero-numel tensor errors (data subject issue?)\n",
    "                return False\n",
    "            if any(t.ndim == 0 for t in arg_values if isinstance(t, torch.Tensor)) or \\\n",
    "                any(t.ndim == 0 for t in kwarg_values.values() if isinstance(t, torch.Tensor)):\n",
    "                # FIXME: Scalar errors (data subject issue?)\n",
    "                return False\n",
    "            if op.name in element_wise_binary_ops and arg_values[0].shape[0] != arg_values[1].shape[0]:\n",
    "                # FIXME: element-wise op when data_subjects_indexed arrays are differently shaped\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        samples = list(filter(supported_by_syft, samples))\n",
    "        wrapped_samples = self.wrap_samples_with_subclass(samples)\n",
    "        for wrapped_sample, sample in zip(wrapped_samples, samples):\n",
    "            # print(\"Sample: \", sample)\n",
    "            # print(\"Wrapped sample: \", wrapped_sample)\n",
    "            # Wrapped\n",
    "            wrapped_args, wrapped_kwargs = wrapped_sample\n",
    "            fn = op.get_op()\n",
    "            raw_output = fn(*wrapped_args, **wrapped_kwargs)\n",
    "            result = self.nptensorwrapper_to_torch(raw_output)\n",
    "\n",
    "            # Non-Wrapped\n",
    "            arg_values = [sample.input] + list(sample.args)\n",
    "            kwarg_values = sample.kwargs\n",
    "            expected = fn(*arg_values, **kwarg_values)\n",
    "            print(result, expected, (result - expected).abs().max())\n",
    "            # print(arg_values)\n",
    "\n",
    "            self.assertTrue(_allclose_with_type_promotion(result, expected, rtol=1e-1, atol=1e-2))\n",
    "\n",
    "    # @ops(op_list=get_op_list([\n",
    "    #     # Backend ops\n",
    "    #     ('sum',''),\n",
    "    #     ('add',''),\n",
    "    #     ('sub',''),\n",
    "    #     ('div',''),\n",
    "    #     ('mul',''),\n",
    "    #     # Decompositions\n",
    "    #     ('sigmoid',''),\n",
    "    #     ('nn.functional.mse_loss',''),\n",
    "    # ]), allowed_dtypes=(torch.float,))\n",
    "    # @skipOps('NPTensorTest', 'test_np_tensor_gradients_parity', {\n",
    "    #     # Forward is already failing for these\n",
    "    #     xfail('add'),\n",
    "    #     xfail('sub'),\n",
    "    #     xfail('rsub',''),\n",
    "    # })\n",
    "    # def test_np_tensor_gradients_parity(self, device, dtype, op):\n",
    "    #     # Tests if tensor and subclass tensor compute the same gradient\n",
    "    #     assert device == 'cpu' and dtype == torch.float\n",
    "\n",
    "    #     samples = op.sample_inputs(device, dtype, requires_grad=True)\n",
    "\n",
    "    #     for sample in samples:\n",
    "    #         arg_values = [sample.input] + list(sample.args)\n",
    "    #         kwarg_values = sample.kwargs\n",
    "\n",
    "    #         subclass_arg_values = tree_map(self.torch_to_nptensorwrapper, arg_values)\n",
    "    #         subclass_kwarg_values = tree_map(self.torch_to_nptensorwrapper, kwarg_values)\n",
    "\n",
    "    #         fn = op.get_op()\n",
    "\n",
    "    #         subclass_out = fn(*subclass_arg_values, **subclass_kwarg_values)\n",
    "    #         out = fn(*arg_values, **kwarg_values)\n",
    "\n",
    "    #         assert isinstance(subclass_out, torch.Tensor), f\"Expected op to return a single tensor, but got: {type(subclass_out)}\"\n",
    "\n",
    "    #         grad_outputs = torch.rand_like(out)\n",
    "    #         subclass_grad_inputs = torch.autograd.grad(subclass_out, subclass_arg_values, grad_outputs=grad_outputs)\n",
    "    #         subclass_grad_inputs = tree_map(self.nptensorwrapper_to_torch, subclass_grad_inputs)\n",
    "\n",
    "    #         grad_inputs = torch.autograd.grad(out, arg_values, grad_outputs=grad_outputs)\n",
    "\n",
    "    #         self.assertEqual(len(subclass_grad_inputs), len(grad_inputs))\n",
    "\n",
    "    #         for subclass_grad_input, grad_input in zip(subclass_grad_inputs, grad_inputs):\n",
    "    #             # print(subclass_grad_input, grad_input, (subclass_grad_input - grad_input).abs().max())\n",
    "    #             # print(arg_values)\n",
    "    #             self.assertTrue(torch.allclose(grad_input, subclass_grad_input))\n",
    "\n",
    "instantiate_device_type_tests(NPTensorTest, globals(), only_for=(\"cpu,\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
